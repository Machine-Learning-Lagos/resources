# Deep Learning Curriculum

## Week 1: Introduction to Deep Learning
**Learning Objectives**: Understand basic deep learning concepts

- What is deep learning
- Multilayer Perceptrons (MLP)
- Loss functions
- Activation functions
- Gradient Descent
- Back propagation
- Learning rate
- Regularization
- Normalization

**Resources**:
- [MIT Introduction to Deep Learning | 6.S191](https://www.youtube.com/watch?v=ErnWZxJovaM&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=1)
- [D2L MLP Chapter](https://d2l.ai/chapter_multilayer-perceptrons/index.html)

## Week 2: Convolutional Neural Networks
**Learning Objective**: Understand CNNs and their architecture

- Convolutions
- Pooling
- Feature extraction
- Classical and Modern CNN architectures

**Resources**:
- [MIT 6.S191: Convolutional Neural Networks](https://www.youtube.com/watch?v=2xqkSUhmmXU&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3)
- [D2L CNN Basics](https://d2l.ai/chapter_convolutional-neural-networks/index.html)
- [D2L Modern CNNs](https://d2l.ai/chapter_convolutional-modern/index.html)

## Week 3: Reinforcement Learning
**Learning Objective**: Master reinforcement learning fundamentals

- Class of learning problems
- Q functions and Networks
- Policy learning algorithms

**Resources**:
- [MIT 6.S191: Reinforcement Learning](https://www.youtube.com/watch?v=8JVRbHAVCws&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=5)
- [D2L Reinforcement Learning](https://d2l.ai/chapter_reinforcement-learning/index.html)

## Week 4: RNNs and LSTMs
**Learning Objective**: Understand sequence-to-sequence techniques

- Sequence Modelling
- Recurrent Neural Networks
- Long Short-Term Memory networks

**Resources**:
- [MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention](https://www.youtube.com/watch?v=dqoEU9Ac3ek&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=2)
- [D2L RNN Basics](https://d2l.ai/chapter_recurrent-neural-networks/index.html)
- [D2L Modern RNNs](https://d2l.ai/chapter_recurrent-modern/index.html)

## Week 5: Attention and Transformers
**Learning Objective**: Master modern transformer architectures

- Attention mechanisms
- Transformers
- Modern transformer architectures
- KV cache

**Resources**:
- [D2L Attention & Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)
- [MIT 6.S191: RNNs, Transformers, and Attention](https://www.youtube.com/watch?v=dqoEU9Ac3ek&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=2)
- [Attention is all you need (Transformer) - Model explanation](https://www.youtube.com/watch?v=bCz4OMemCcA&t=5s)
- [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm](https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2s)

## Week 6: Generative Deep Learning
**Learning Objective**: Understand generative models

- Variational Autoencoders (VAEs)
- Generative Adversarial Networks (GANs)
- Diffusion Models

**Resources**:
- [Variational Autoencoder - Model, ELBO, loss function](https://www.youtube.com/watch?v=iwEzwTTalbg)
- [Understanding GANs](https://www.youtube.com/watch?v=RAa55G-oEuk)
- [L6 Diffusion Models (SP24)](https://www.youtube.com/watch?v=DsEDMjdxOv4)